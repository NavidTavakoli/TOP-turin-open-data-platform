name: ETL - Reddit Torino (latest 50, every 2h, title+selftext)

on:
  schedule:
    - cron: "7 */2 * * *"   # Every 2 hours (UTC); minute 7 to avoid traffic
  workflow_dispatch: {}

jobs:
  reddit:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    concurrency:
      group: etl-reddit
      cancel-in-progress: true

    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Log schedule timing
        run: |
          echo "event_name=${{ github.event_name }}"
          echo "run_started_at=${{ github.run_started_at }}"
          date -u "+now_utc=%Y-%m-%dT%H:%M:%SZ"

      - name: Sanity - required secrets
        run: |
          test -n "${{ secrets.POSTGREST_URL }}"        || (echo "POSTGREST_URL secret missing" && exit 1)
          test -n "${{ secrets.SERVICE_KEY }}"          || (echo "SERVICE_KEY secret missing" && exit 1)
          test -n "${{ secrets.REDDIT_CLIENT_ID }}"     || (echo "REDDIT_CLIENT_ID missing" && exit 1)
          test -n "${{ secrets.REDDIT_CLIENT_SECRET }}" || (echo "REDDIT_CLIENT_SECRET missing" && exit 1)

      - name: Fetch 50 & Keep only this batch (title + selftext)
        env:
          POSTGREST_URL:        ${{ secrets.POSTGREST_URL }}
          SERVICE_KEY:          ${{ secrets.SERVICE_KEY }}
          REDDIT_CLIENT_ID:     ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
        run: |
          node - <<'NODE'
          // Node 20 → global fetch is available
          const env = process.env;
          const UA  = "TOP-turin-open-data-platform/1.0 (+https://github.com/<user>/<repo>; contact: you@example.com)";

          async function getToken() {
            const basic = Buffer.from(`${env.REDDIT_CLIENT_ID}:${env.REDDIT_CLIENT_SECRET}`).toString("base64");
            const r = await fetch("https://www.reddit.com/api/v1/access_token", {
              method: "POST",
              headers: {
                "Authorization": `Basic ${basic}`,
                "User-Agent": UA,
                "Content-Type": "application/x-www-form-urlencoded"
              },
              body: new URLSearchParams({ grant_type: "client_credentials" })
            });
            if (!r.ok) throw new Error(`oauth token ${r.status}: ${await r.text()}`);
            const j = await r.json();
            if (!j.access_token) throw new Error("no access_token");
            return j.access_token;
          }

          // For title we only clean up spaces;
          // leave selftext raw to preserve Markdown/Formatting.
          const cleanTitle = s => (s ?? "").replace(/\s+/g, " ").trim();

          (async () => {
            // 1) Get token and fetch the last 50 posts
            const token = await getToken();
            const res = await fetch("https://oauth.reddit.com/r/torino/new?limit=50&raw_json=1", {
              headers: { "Authorization": `Bearer ${token}`, "User-Agent": UA, "Accept": "application/json" }
            });
            if (!res.ok) throw new Error(`Reddit fetch ${res.status}: ${await res.text()}`);

            const j = await res.json();
            const items = Array.isArray(j?.data?.children) ? j.data.children : [];

            // 2) Map to the database schema (title + selftext)
            const rows = items.map(c => {
              const d = c.data || {};
              return {
                post_id:   String(d.id || ""),
                ts:        new Date((d.created_utc || 0) * 1000).toISOString(),
                title:     cleanTitle(d.title || ""),
                selftext:  d.selftext ?? null, // Raw Markdown
                permalink: d.permalink ? `https://www.reddit.com${d.permalink}` : null
              };
            }).filter(r => r.post_id && (r.title || r.selftext));

            const base = env.POSTGREST_URL.replace(/\/+$/,"");
            const headers = {
              apikey: env.SERVICE_KEY,
              Authorization: `Bearer ${env.SERVICE_KEY}`,
              "Content-Type": "application/json",
              "Content-Profile": "api"
            };

            // Keep previous data if no items fetched (avoid empty site)
            if (!rows.length) {
              console.log("No rows from Reddit this run. Skipping upsert & prune.");
              return;
            }

            // 3) Batch upsert (avoids duplicate inserts)
            const ins = await fetch(`${base}/reddit_torino_posts`, {
              method: "POST",
              headers: { ...headers, Prefer: "resolution=ignore-duplicates" },
              body: JSON.stringify(rows)
            });
            const insBody = await ins.text();
            console.log(`Upsert: ${ins.status}; sent=${rows.length}; resp:`, insBody || "<no-body>");
            if (!ins.ok) throw new Error("insert failed");

            // 4) Delete all records that are not among these current 50 (WHERE required → OK)
            const ids = rows.map(x => x.post_id);
            const delUrl = new URL(`${base}/reddit_torino_posts`);
            // Quote ids for safety
            delUrl.searchParams.set("post_id", `not.in.(${ids.map(s => `"${s}"`).join(",")})`);

            const del = await fetch(delUrl, { method: "DELETE", headers });
            const delBody = await del.text();
            console.log(`Pruned others: ${del.status}; resp:`, delBody || "<no-body>");
            if (!del.ok) throw new Error(`prune failed ${del.status}`);
          })().catch(e => { console.error(e); process.exit(1); });
          NODE
